from PyQt5 import QtWidgets, QtCore, QtGui
from PyQt5.QtCore import Qt
import cv2, os, time
from threading import Thread
import threading
import tempfile
from pathlib import Path
from PyQt5.QtWidgets import QProgressDialog
import numpy as np
import deep_sort.deep_sort.deep_sort as ds
from PyQt5.QtGui import QPixmap,QImage
from PyQt5.QtWidgets import QLabel,QFileDialog
# ä¸ç„¶æ¯æ¬¡YOLOå¤„ç†éƒ½ä¼šè¾“å‡ºè°ƒè¯•ä¿¡æ¯
os.environ['YOLO_VERBOSE'] = 'False'
from ultralytics import YOLO 
def putTextWithBackground(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1, text_color=(255, 255, 255), bg_color=(0, 0, 0), thickness=1):
    """ç»˜åˆ¶å¸¦æœ‰èƒŒæ™¯çš„æ–‡æœ¬ã€‚

    :param img: è¾“å…¥å›¾åƒã€‚
    :param text: è¦ç»˜åˆ¶çš„æ–‡æœ¬ã€‚
    :param origin: æ–‡æœ¬çš„å·¦ä¸Šè§’åæ ‡ã€‚
    :param font: å­—ä½“ç±»å‹ã€‚
    :param font_scale: å­—ä½“å¤§å°ã€‚
    :param text_color: æ–‡æœ¬çš„é¢œè‰²ã€‚
    :param bg_color: èƒŒæ™¯çš„é¢œè‰²ã€‚
    :param thickness: æ–‡æœ¬çš„çº¿æ¡åšåº¦ã€‚
    """
    # è®¡ç®—æ–‡æœ¬çš„å°ºå¯¸
    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)

    # ç»˜åˆ¶èƒŒæ™¯çŸ©å½¢
    bottom_left = origin
    top_right = (origin[0] + text_width, origin[1] - text_height - 5)  # å‡å»5ä»¥ç•™å‡ºä¸€äº›è¾¹è·
    cv2.rectangle(img, bottom_left, top_right, bg_color, -1)

    # åœ¨çŸ©å½¢ä¸Šç»˜åˆ¶æ–‡æœ¬
    text_origin = (origin[0], origin[1] - 5)  # ä»å·¦ä¸Šè§’çš„ä½ç½®å‡å»5æ¥ç•™å‡ºä¸€äº›è¾¹è·
    cv2.putText(img, text, text_origin, font, font_scale, text_color, thickness, lineType=cv2.LINE_AA)
    
def extract_detections(results, detect_class):
    """
    ä»æ¨¡å‹ç»“æœä¸­æå–å’Œå¤„ç†æ£€æµ‹ä¿¡æ¯ã€‚
    - results: YoloV8æ¨¡å‹é¢„æµ‹ç»“æœ,åŒ…å«æ£€æµ‹åˆ°çš„ç‰©ä½“çš„ä½ç½®ã€ç±»åˆ«å’Œç½®ä¿¡åº¦ç­‰ä¿¡æ¯ã€‚
    - detect_class: éœ€è¦æå–çš„ç›®æ ‡ç±»åˆ«çš„ç´¢å¼•ã€‚
    å‚è€ƒ: https://docs.ultralytics.com/modes/predict/#working-with-results
    """
    
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„äºŒç»´numpyæ•°ç»„ï¼Œç”¨äºå­˜æ”¾æ£€æµ‹åˆ°çš„ç›®æ ‡çš„ä½ç½®ä¿¡æ¯
    # å¦‚æœè§†é¢‘ä¸­æ²¡æœ‰éœ€è¦æå–çš„ç›®æ ‡ç±»åˆ«ï¼Œå¦‚æœä¸åˆå§‹åŒ–ï¼Œä¼šå¯¼è‡´trackeræŠ¥é”™
    detections = np.empty((0, 4)) 
    
    confarray = [] # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜æ”¾æ£€æµ‹åˆ°çš„ç›®æ ‡çš„ç½®ä¿¡åº¦ã€‚
    
    # éå†æ£€æµ‹ç»“æœ
    # å‚è€ƒï¼šhttps://docs.ultralytics.com/modes/predict/#working-with-results
    for r in results:
        for box in r.boxes:
            # å¦‚æœæ£€æµ‹åˆ°çš„ç›®æ ‡ç±»åˆ«ä¸æŒ‡å®šçš„ç›®æ ‡ç±»åˆ«ç›¸åŒ¹é…ï¼Œæå–ç›®æ ‡çš„ä½ç½®ä¿¡æ¯å’Œç½®ä¿¡åº¦
            if box.cls[0].int() == detect_class:
                x1, y1, x2, y2 = box.xywh[0].int().tolist() # æå–ç›®æ ‡çš„ä½ç½®ä¿¡æ¯ï¼Œå¹¶ä»tensorè½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨ã€‚
                conf = round(box.conf[0].item(), 2) # æå–ç›®æ ‡çš„ç½®ä¿¡åº¦ï¼Œä»tensorä¸­å–å‡ºæµ®ç‚¹æ•°ç»“æœï¼Œå¹¶å››èˆäº”å…¥åˆ°å°æ•°ç‚¹åä¸¤ä½ã€‚
                detections = np.vstack((detections, np.array([x1, y1, x2, y2]))) # å°†ç›®æ ‡çš„ä½ç½®ä¿¡æ¯æ·»åŠ åˆ°detectionsæ•°ç»„ä¸­ã€‚
                confarray.append(conf) # å°†ç›®æ ‡çš„ç½®ä¿¡åº¦æ·»åŠ åˆ°confarrayåˆ—è¡¨ä¸­ã€‚
    return detections, confarray # è¿”å›æå–å‡ºçš„ä½ç½®ä¿¡æ¯å’Œç½®ä¿¡åº¦ã€‚

# è§†é¢‘å¤„ç†
def detect_and_track(input_path: str, output_path: str, detect_class: int, model, tracker) -> Path:
    """
    å¤„ç†è§†é¢‘ï¼Œæ£€æµ‹å¹¶è·Ÿè¸ªç›®æ ‡ã€‚
    - input_path: è¾“å…¥è§†é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
    - output_path: å¤„ç†åè§†é¢‘ä¿å­˜çš„è·¯å¾„ã€‚
    - detect_class: éœ€è¦æ£€æµ‹å’Œè·Ÿè¸ªçš„ç›®æ ‡ç±»åˆ«çš„ç´¢å¼•ã€‚
    - model: ç”¨äºç›®æ ‡æ£€æµ‹çš„æ¨¡å‹ã€‚
    - tracker: ç”¨äºç›®æ ‡è·Ÿè¸ªçš„æ¨¡å‹ã€‚
    """
    cap = cv2.VideoCapture(input_path)  # ä½¿ç”¨OpenCVæ‰“å¼€è§†é¢‘æ–‡ä»¶ã€‚
    if not cap.isOpened():  # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æˆåŠŸæ‰“å¼€ã€‚
        print(f"Error opening video file {input_path}")
        return None
    
    fps = cap.get(cv2.CAP_PROP_FPS)  # è·å–è§†é¢‘çš„å¸§ç‡
    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))) # è·å–è§†é¢‘çš„åˆ†è¾¨ç‡ï¼ˆå®½åº¦å’Œé«˜åº¦ï¼‰ã€‚
    output_video_path = Path(output_path) / "output.avi" # è®¾ç½®è¾“å‡ºè§†é¢‘çš„ä¿å­˜è·¯å¾„ã€‚

    # è®¾ç½®è§†é¢‘ç¼–ç æ ¼å¼ä¸ºXVIDæ ¼å¼çš„aviæ–‡ä»¶
    # å¦‚æœéœ€è¦ä½¿ç”¨h264ç¼–ç æˆ–è€…éœ€è¦ä¿å­˜ä¸ºå…¶ä»–æ ¼å¼ï¼Œå¯èƒ½éœ€è¦ä¸‹è½½openh264-1.8.0
    # ä¸‹è½½åœ°å€ï¼šhttps://github.com/cisco/openh264/releases/tag/v1.8.0
    # ä¸‹è½½å®Œæˆåå°†dllæ–‡ä»¶æ”¾åœ¨å½“å‰æ–‡ä»¶å¤¹å†…
    fourcc = cv2.VideoWriter_fourcc(*"XVID")
    output_video = cv2.VideoWriter(output_video_path.as_posix(), fourcc, fps, size, isColor=True) # åˆ›å»ºä¸€ä¸ªVideoWriterå¯¹è±¡ç”¨äºå†™è§†é¢‘ã€‚

    # å¯¹æ¯ä¸€å¸§å›¾ç‰‡è¿›è¡Œè¯»å–å’Œå¤„ç†
    while True:
        success, frame = cap.read() # é€å¸§è¯»å–è§†é¢‘ã€‚
        
        # å¦‚æœè¯»å–å¤±è´¥ï¼ˆæˆ–è€…è§†é¢‘å·²å¤„ç†å®Œæ¯•ï¼‰ï¼Œåˆ™è·³å‡ºå¾ªç¯ã€‚
        if not (success):
            break

        # ä½¿ç”¨YoloV8æ¨¡å‹å¯¹å½“å‰å¸§è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚
        results = model(frame, stream=True)

        # ä»é¢„æµ‹ç»“æœä¸­æå–æ£€æµ‹ä¿¡æ¯ã€‚
        detections, confarray = extract_detections(results, detect_class)

        # ä½¿ç”¨deepsortæ¨¡å‹å¯¹æ£€æµ‹åˆ°çš„ç›®æ ‡è¿›è¡Œè·Ÿè¸ªã€‚
        resultsTracker = tracker.update(detections, confarray, frame)
        
        for x1, y1, x2, y2, Id in resultsTracker:
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2]) # å°†ä½ç½®ä¿¡æ¯è½¬æ¢ä¸ºæ•´æ•°ã€‚

            # ç»˜åˆ¶bounding boxå’Œæ–‡æœ¬
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)
            putTextWithBackground(frame, str(int(Id)), (max(-10, x1), max(40, y1)), font_scale=1.5, text_color=(255, 255, 255), bg_color=(255, 0, 255))

        output_video.write(frame)  # å°†å¤„ç†åçš„å¸§å†™å…¥åˆ°è¾“å‡ºè§†é¢‘æ–‡ä»¶ä¸­ã€‚
            
    output_video.release()  # é‡Šæ”¾VideoWriterå¯¹è±¡ã€‚
    cap.release()  # é‡Šæ”¾è§†é¢‘æ–‡ä»¶ã€‚
    
    print(f'output dir is: {output_video_path}')
    return output_video_path
class MWindow(QtWidgets.QMainWindow):

    def __init__(self):

        super().__init__()

        # è®¾ç½®ç•Œé¢
        self.setupUI()

        self.videoBtn.clicked.connect(self.choose_video)
        self.startBtn.clicked.connect(self.play_video)

        # å®šä¹‰å®šæ—¶å™¨ï¼Œç”¨äºæ§åˆ¶æ˜¾ç¤ºè§†é¢‘çš„å¸§ç‡
        # self.timer_camera = QtCore.QTimer()
        # å®šæ—¶åˆ°äº†ï¼Œå›è°ƒ self.show_camera
        # self.timer_camera.timeout.connect(self.show_camera)


        # å¯åŠ¨å¤„ç†è§†é¢‘å¸§ç‹¬ç«‹çº¿ç¨‹
        # Thread(target=self.frameAnalyzeThreadFunc,daemon=True).start()
                # åˆ›å»ºè¿›åº¦å¯¹è¯æ¡†


    def setupUI(self):
        self.resize(1200, 800)
        self.setWindowTitle('åŸºäºæ·±åº¦å­¦ä¹ çš„å»ºç­‘å·¥äººè§†é¢‘è·Ÿè¸ªæŠ€æœ¯')
        self.setStyleSheet("QMainWindow { font-size: 16pt; }")  # è®¾ç½®æ ‡é¢˜å­—ä½“å¤§å°

        # central Widget
        centralWidget = QtWidgets.QWidget(self)
        self.setCentralWidget(centralWidget)

        # central Widget é‡Œé¢çš„ ä¸» layout
        mainLayout = QtWidgets.QVBoxLayout(centralWidget)

        # ç•Œé¢çš„ä¸ŠåŠéƒ¨åˆ† : å›¾å½¢å±•ç¤ºéƒ¨åˆ†
        topLayout = QtWidgets.QHBoxLayout()
        
        # è§†é¢‘åŸå§‹
        videoLabelLayout1 = QtWidgets.QVBoxLayout()
        self.label_ori_video = QtWidgets.QLabel(self)
        self.label_ori_video.setMinimumSize(520, 400)
        self.label_ori_video.setStyleSheet('border: 1px solid #D7E2F9;')
        label_ori_video_title = QtWidgets.QLabel("è¿½è¸ªåŸè§†é¢‘")
        label_ori_video_title.setAlignment(Qt.AlignCenter)
        label_ori_video_title.setStyleSheet("font-size: 14pt; font-weight: bold;")
        videoLabelLayout1.addWidget(self.label_ori_video)
        videoLabelLayout1.addWidget(label_ori_video_title)
        topLayout.addLayout(videoLabelLayout1)

        # è§†é¢‘å¤„ç†å
        videoLabelLayout2 = QtWidgets.QVBoxLayout()
        self.label_ori_video1 = QtWidgets.QLabel(self)
        self.label_ori_video1.setMinimumSize(520, 400)
        self.label_ori_video1.setStyleSheet('border: 1px solid #D7E2F9;')
        label_ori_video1_title = QtWidgets.QLabel("è¿½è¸ªåè§†é¢‘")
        label_ori_video1_title.setAlignment(Qt.AlignCenter)
        label_ori_video1_title.setStyleSheet("font-size: 14pt; font-weight: bold;")
        videoLabelLayout2.addWidget(self.label_ori_video1)
        videoLabelLayout2.addWidget(label_ori_video1_title)
        topLayout.addLayout(videoLabelLayout2)
        
        mainLayout.addLayout(topLayout)

        # ç•Œé¢ä¸‹åŠéƒ¨åˆ†ï¼š è¾“å‡ºæ¡† å’Œ æŒ‰é’®
        groupBox = QtWidgets.QGroupBox(self)
        bottomLayout = QtWidgets.QHBoxLayout(groupBox)
        self.textLog = QtWidgets.QTextBrowser()
        self.textLog.setStyleSheet("font-size: 18pt;")  # è®¾ç½®æ–‡æœ¬æ¡†å†…å­—ä½“å¤§å°
        bottomLayout.addWidget(self.textLog, 1)  # æ”¹å˜æ–‡æœ¬æ¡†çš„æ¯”ä¾‹å› å­

        mainLayout.addWidget(groupBox)

        btnLayout = QtWidgets.QVBoxLayout()
        self.videoBtn = QtWidgets.QPushButton('ğŸï¸è§†é¢‘æ–‡ä»¶')
        self.startBtn = QtWidgets.QPushButton('â–¶å¼€å§‹æ’­æ”¾')
        btnLayout.addWidget(self.videoBtn)
        btnLayout.addWidget(self.startBtn)
        bottomLayout.addLayout(btnLayout, 0)  # å‡å°æŒ‰é’®éƒ¨åˆ†çš„æ¯”ä¾‹å› å­
    # def play_video(self):
    #     cap = cv2.VideoCapture('output\output.avi')
    #     if not cap.isOpened():
    #         self.textLog.append("Error: Unable to open video.")
    #         return

    #     while True:
    #         ret, frame = cap.read()
    #         if not ret:
    #             break

    #         # å°†OpenCVçš„å›¾åƒæ ¼å¼è½¬æ¢ä¸ºQtæ”¯æŒçš„æ ¼å¼
    #         height, width, channel = frame.shape
    #         bytesPerLine = 3 * width
    #         qImg = QImage(frame.data, width, height, bytesPerLine, QImage.Format_RGB888).rgbSwapped()
    #         pixmap = QPixmap.fromImage(qImg)

    #         # åœ¨æ ‡ç­¾ä¸­æ˜¾ç¤ºè§†é¢‘å¸§
    #         self.label_ori_video.setPixmap(pixmap.scaled(self.label_ori_video.size(), Qt.KeepAspectRatio))

    #         # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œä»¥ä¾¿å¯ä»¥è§‚å¯Ÿè§†é¢‘å¸§
    #         cv2.waitKey(30)

    #     cap.release()
    def play_video(self):
        # æ‰“å¼€ç¬¬ä¸€ä¸ªè§†é¢‘
        cap1 = cv2.VideoCapture('output\output.avi')
        if not cap1.isOpened():
            self.textLog.append("Error: Unable to open first video.")
            return

        # æ‰“å¼€ç¬¬äºŒä¸ªè§†é¢‘
        cap2 = cv2.VideoCapture(self.video_file_path)
        if not cap2.isOpened():
            self.textLog.append("Error: Unable to open second video.")
            return

        while True:
            # è¯»å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„å¸§
            ret1, frame1 = cap1.read()
            if not ret1:
                break

            # è¯»å–ç¬¬äºŒä¸ªè§†é¢‘çš„å¸§
            ret2, frame2 = cap2.read()
            if not ret2:
                break

            # å°†ç¬¬ä¸€ä¸ªè§†é¢‘å¸§è½¬æ¢ä¸ºQtæ”¯æŒçš„æ ¼å¼å¹¶åœ¨label_ori_video1ä¸­æ˜¾ç¤º
            height1, width1, channel1 = frame1.shape
            bytesPerLine1 = 3 * width1
            qImg1 = QImage(frame1.data, width1, height1, bytesPerLine1, QImage.Format_RGB888).rgbSwapped()
            pixmap1 = QPixmap.fromImage(qImg1)
            self.label_ori_video1.setPixmap(pixmap1.scaled(self.label_ori_video1.size(), Qt.KeepAspectRatio))

            # å°†ç¬¬äºŒä¸ªè§†é¢‘å¸§è½¬æ¢ä¸ºQtæ”¯æŒçš„æ ¼å¼å¹¶åœ¨label_ori_videoä¸­æ˜¾ç¤º
            height2, width2, channel2 = frame2.shape
            bytesPerLine2 = 3 * width2
            qImg2 = QImage(frame2.data, width2, height2, bytesPerLine2, QImage.Format_RGB888).rgbSwapped()
            pixmap2 = QPixmap.fromImage(qImg2)
            self.label_ori_video.setPixmap(pixmap2.scaled(self.label_ori_video.size(), Qt.KeepAspectRatio))

            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œä»¥ä¾¿å¯ä»¥è§‚å¯Ÿè§†é¢‘å¸§
            cv2.waitKey(30)

        # é‡Šæ”¾è§†é¢‘å¯¹è±¡
        cap1.release()
        cap2.release()

    # def choose_video(self):
    #     # æ‰“å¼€æ–‡ä»¶å¯¹è¯æ¡†
    #     options = QFileDialog.Options()
    #     file_path, _ = QFileDialog.getOpenFileName(self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi)", options=options)
    #     if file_path:
    #         # å¦‚æœç”¨æˆ·é€‰æ‹©äº†æ–‡ä»¶ï¼Œå°†æ–‡ä»¶è·¯å¾„æ˜¾ç¤ºåœ¨è¾“å‡ºæ¡†ä¸­
    #         self.textLog.append(f"é€‰æ‹©çš„è§†é¢‘æ–‡ä»¶ï¼š{file_path}")
    #         self.video_file_path = file_path  # å°†æ–‡ä»¶è·¯å¾„ä¿å­˜åœ¨å®ä¾‹å˜é‡ä¸­
    #         detect_and_track(self.video_file_path, output_path, detect_class, model, tracker)
    #         self.textLog.append("è§†é¢‘å¤„ç†å®Œæˆ")
    def choose_video(self):
        # æ‰“å¼€æ–‡ä»¶å¯¹è¯æ¡†
        options = QFileDialog.Options()
        file_path, _ = QFileDialog.getOpenFileName(self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶", "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi)", options=options)
        if file_path:
            # å¦‚æœç”¨æˆ·é€‰æ‹©äº†æ–‡ä»¶ï¼Œå°†æ–‡ä»¶è·¯å¾„æ˜¾ç¤ºåœ¨è¾“å‡ºæ¡†ä¸­
            self.textLog.append(f"å·²é€‰æ‹©çš„è§†é¢‘æ–‡ä»¶ï¼š{file_path}")
            self.video_file_path = file_path  # å°†æ–‡ä»¶è·¯å¾„ä¿å­˜åœ¨å®ä¾‹å˜é‡ä¸­
            # åˆ›å»ºçº¿ç¨‹æ¥æ‰§è¡Œ detect_and_track å‡½æ•°
            thread = threading.Thread(target=self.execute_detect_and_track)
            thread.start()

    def execute_detect_and_track(self):
        # æ‰§è¡Œ detect_and_track å‡½æ•°
        detect_and_track(self.video_file_path, output_path, detect_class, model, tracker)
        # å¤„ç†å®Œæˆåå‘self.textLogæ·»åŠ æ—¥å¿—
        self.textLog.append("ç¨‹åºé¢„å¤„ç†å®Œæˆ")



if __name__ == "__main__":
        # æŒ‡å®šè¾“å…¥è§†é¢‘çš„è·¯å¾„ã€‚
    ######
    input_path = "test.mp4"
    ######

    # è¾“å‡ºæ–‡ä»¶å¤¹ï¼Œé»˜è®¤ä¸ºç³»ç»Ÿçš„ä¸´æ—¶æ–‡ä»¶å¤¹è·¯å¾„
    output_path = 'output'  # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ç”¨äºå­˜æ”¾è¾“å‡ºè§†é¢‘ã€‚

    # åŠ è½½yoloV8æ¨¡å‹æƒé‡
    model = YOLO("yolov8n.pt")

    # è®¾ç½®éœ€è¦æ£€æµ‹å’Œè·Ÿè¸ªçš„ç›®æ ‡ç±»åˆ«
    # yoloV8å®˜æ–¹æ¨¡å‹çš„ç¬¬ä¸€ä¸ªç±»åˆ«ä¸º'person'
    detect_class = 0
    # print(f"detecting {model.names[detect_class]}") # model.namesè¿”å›æ¨¡å‹æ‰€æ”¯æŒçš„æ‰€æœ‰ç‰©ä½“ç±»åˆ«

    # åŠ è½½DeepSortæ¨¡å‹
    tracker = ds.DeepSort("deep_sort/deep_sort/deep/checkpoint/ckpt.t7")

    app = QtWidgets.QApplication([])
    window = MWindow()
    window.show()
    app.exec()